#shortened decriptions and tags i will be using for parameters
#      meme (minimize for efficiency,maximize for effectiveness)
#      anti-meme (maximize for efficiency,minimize for effectiveness), should come up rarely
#      prime (must be a prime number)
#      note (do not ignore what comes after it)
#      mtm  (more the merrier;less significant figures more efficiency,more significant figures more effectiveness)

#useful basic math operators
# * means multiply % means modulo ** means to the power of +,- the same / division
# // floor division removes decimal points from answer,by always lowering the value to the next whole number e.g. -4.15->-5 , 3.87->3
# == equality check e.g. a==a+1 is not true and a==a is true,!= inequality check,a!=a+1 is true and a!= is not true similiar to <>
# > if greater than then true a+1>a is true a>a+! untrue, < if less than true a+1<a untrue a<a+1 tue,>= greater and equals true,<= less than equals true
# = assigns value on right to the variable on the left e.g. b = a + 1 makes b a + 1
# += , c += a is the same as c = c + a
# -= , c -= a is the same as c = c - a, *= , c *= a is the same as c = c * a, /=, %=,**=,//=, you get the idea
# order of sums: brakets,exponents,multiply divide modular arithmatic floor division,addition subraction,comparison operators e.g. <,equality operators e.g. ==, finally the funky shortened sum things e.g. +=

# simple rng,note can only produce positive random numbers
def randnm(x,m) #x can be any pseudo random number,dice the time in milliseconds etc.,m is maximum value from rng
  #parameters of rng
  x=       #meme;this is essentially a seed for the rng
  m=       #note;m should be the smallest value here
  global rngout
  a = (x * rngout) + (x  * m)
  rngout = a % m
  #rngout is the random number produced(note give an initial value as a global variable to r as a  global variable think of it as a second seed)

#reLU, most efficient activation algorithm for hidden layers
def reLU(x,a)
  global reLUout
  if x > 0
    reLUout = x
  else
    a = 0.01  #zero for normal reLU, small number for leaky reLU,keep it as a learned parameter for Para Relu(effective not efficient,evolution may be the easiest way to implement)
    reLUout = x*a #you can change this formula if you want something different where x is negative, you can study how different functions here affect things

def eulerno(x) #note:run this function once to give a value to euler's number note:both e and x are meme
  global eulern #euler's number or e
  eulern = (1+1/x)**x

def sigmoid(x)#good output function for classsification problems with multiple outputs)
  global eulern #requires euler's number
  global sigout 
  var = 1/30 #anti-meme, can be adjusted with learning or evolution
  sigout = 1 / ( 1 + eulern ** ( var * x * (-1) ) )

def ezcost(x,y)#x is the output,y is target output
  global cost
  cost = ( x - y ) ** 2

#creates an input layer
input = []
def inputgen(x)#adds elements to the input layer, x is the number of elements in input layer
  for length in range(x)
    input.append(elmnt)
    
def inputwrite(x,y)#assigns a value y to element x of the input,note:lists start at 0 so first element is 0 2nd 1 one etc.
  global input
  input[x] = y

#hidden layers,i will store these as a list of lists, i know that it is not efficient but if translated to arrays in c there will be no problem    
def hiddenlayergen(x)
  numofnu = []#create a list of the number of neurons in each layer here
  for number in range(x)
    global hiddenlayers
    hiddenlayers.append([])
    for neurons in range(numofnu[x])
      hiddenlayers[x].append([])
      
  
#creates an output layer
output = []
def outputgen(x)#adds elements to the output layer, x is the number of elements in output layer
  for length in range(x)
    output.append(elmnt)
    
def weightgen  
  global weights
  global hiddenlayers
  global input         #the number of weights depends on inputs and hidden layers
  weights = []
  k = len(hiddenlayers)+1
  for amount in range(k)
    weights.append([])
  for inputweights in range(len(input))
    weights[0].append([])
    for
  
  
  
  
  
  
  
