#shortened decriptions and tags i will be using for parameters

#      meme (minimize for efficiency,maximize for effectiveness)

#      anti-meme (maximize for efficiency,minimize for effectiveness), should come up rarely

#      prime (must be a prime number)

#      note (do not ignore what comes after it)

#      mtm  (more the merrier;less significant figures more efficiency,more significant figures more effectiveness)






#useful basic math operators

# * means multiply % means modulo ** means to the power of +,- the same / division

# // floor division removes decimal points from answer,by always lowering the value to the next whole number e.g. -4.15->-5 , 3.87->3

# == equality check e.g. a==a+1 is not true and a==a is true,!= inequality check,a!=a+1 is true and a!= is not true similiar to <>

# > if greater than then true a+1>a is true a>a+! untrue, < if less than true a+1<a untrue a<a+1 tue,>= greater and equals true,<= less than equals true

# = assigns value on right to the variable on the left e.g. b = a + 1 makes b a + 1

# += , c += a is the same as c = c + a

# -= , c -= a is the same as c = c - a, *= , c *= a is the same as c = c * a, /=, %=,**=,//=, you get the idea

# order of sums: brakets,exponents,multiply divide modular arithmatic floor division,addition subraction,comparison operators e.g. <,equality operators e.g. ==, finally the funky shortened sum things e.g. +=






# simple rng,note: can only produce positive random numbers
def randnm(x,m)                                   #x can be any pseudo random number,dice the time in milliseconds etc.,m is maximum value from rng
#parameters of rng
  x=                                              #meme;this is essentially a seed for the rng
  m=                                              #note;m should be the smallest value here
  global rngout
  a = (x * rngout) + (x  * m)
  rngout = a % m
                                                  #rngout is the random number produced(note give an initial value as a global variable to r as a  global variable think of it as a second seed)






#reLU, most efficient activation algorithm for hidden layers
def reLU(x,a)
  global reLUout
  if x > 0
    reLUout = x
  else
    a = 0.01                                      #zero for normal reLU, small number for leaky reLU,keep it as a learned parameter for Para Relu(effective not efficient,evolution may a good way to implement if there are  other parameterss that would also evolve)
    reLUout = x*a                                 #you can change this formula if you want something different where x is negative, you can study how different functions here affect things





def eulerno(x)                                    #note:run this function once to give a value to euler's number note:both e and x are meme
  global eulern                                   #euler's number or e
  eulern = (1+1/x)**x

def sigmoid(x)                                    #good output function for classsification problems with multiple outputs)
  global eulern                                   #requires euler's number
  global sigout 
  var = 1/30                                      #anti-meme, can be adjusted with learning or evolution,note: var > 1 is risky and possibly very ineffective,can ruin your entire network
  sigout = 1 / ( 1 + ( eulern ** ( var * x * (-1) ) ) )






def ezcost(x,y)                                   #x is the output,y is target output
  global cost
  cost = ( x - y ) ** 2






#creates an input layer
input = []
def inputgen(x)                                   #adds elements to the input layer, x is the number of elements in input layer
  for length in range(x)
    input.append(0)
    





def inputwrite(x,y)                               #assigns a value y to element x of the input,note:lists start at 0 so first element is 0 2nd 1 one etc.
  global input
  input[x] = y






#hidden layers,i will store these as a list of lists, i know that it is not efficient but if translated to arrays in c there will be no problem    
def hiddenlayergen(x)
  numofnu = []                                    #create a list of the number of neurons in each layer here
  for number in range(x)
    global hiddenlayers
    hiddenlayers.append([])
    for neurons in range(numofnu[x])
      hiddenlayers[x].append(0)
      
  






#creates an output layer
output = []
def outputgen(x)                                  #adds elements to the output layer, x is the number of elements in output layer
  for length in range(x)
    output.append(0)
    





def weightgen()                                     #creates a list of list of weights aka fustration 
  global weights
  global hiddenlayers
  global input                                    #the number of weights depends on inputs and hidden layers
  weights = []
  k = len(hiddenlayers)+1                         #there is one extra layer of weights because of output
  for amount in range(k)
    weights.append([])                            #creates the number of sets of weights
  for inputweights in range(len(input))           # the special set of weights from input to first hidden layer
    weights[0].append([])                         #adds a row of weights for each input
    for inputmulti in range(len(hiddenlayers[0]))
      weights[0][inputweights].append(0)          #adds weights to each row based on hidden layer
  for x in range(len(hiddenlayers))
     y = x + 1
     if y < len(hiddenlayers)
      for z in range(len(hiddenlayers[x]))         
        weights[y].append([])                     #each matrix of weights has a number of rows
        for a in range(len(hiddenlayers[y]))
           weights[y][z].append(0)                #each row has a number of weights
     else
       for z in range(len(hiddenlayers[-1])) 
        weights[-1].append([])                    #special case for output 
        global output
        for a in range(len(output))
           weights[-1][z].append(0)
        
  
def biasgen()                                       #generates the list of biases
  global bias
  bias = []
  global hiddenlayers
  global output
  bias = hiddenlayers.copy()
  bias.append(output)

def squish()
  global placeholder
  global placeholderz
  for x in range(len(placeholder[0]))
    for y in range(len(placeholder))
      placeholderz[x] = placeholder[y][x] + placeholderz[x]
def fireactivation()
  global input
  global hiddenlayers
  global weights
  global bias
  global placeholder
  global placeholderz
  placeholderz = []
  placeholder = []
  for x in range(len(input))
    placeholder.append([])
    placeholderz.append([])
    for weighted in range(len(hiddenlayer[0])
      placeholder[x].append(input[x] * weights[0][x][weighted])
  squish()
  for x in range(len(placeholderz))
    placeholderz[x] = placeholderz[x] + bias[0][x]
    
      
  
    
  
