#shortened decriptions and tags i will be using for parameters
#      meme (minimize for efficiency,maximize for effectiveness)
#      anti-meme (maximize for efficiency,minimize for effectiveness), should come up rarely
#      prime (must be a prime number)
#      note (do not ignore what comes after it)
#      mtm  (more the merrier;less significant figures more efficiency,more significant figures more effectiveness)

#useful basic math operators
# * means multiply % means modulo ** means to the power of +,- the same / division
# // floor division removes decimal points from answer,by always lowering the value to the next whole number e.g. -4.15->-5 , 3.87->3
# == equality check e.g. a==a+1 is not true and a==a is true,!= inequality check,a!=a+1 is true and a!= is not true similiar to <>
# > if greater than then true a+1>a is true a>a+! untrue, < if less than true a+1<a untrue a<a+1 tue,>= greater and equals true,<= less than equals true
# = assigns value on right to the variable on the left e.g. b = a + 1 makes b a + 1
# += , c += a is the same as c = c + a
# -= , c -= a is the same as c = c - a, *= , c *= a is the same as c = c * a, /=, %=,**=,//=, you get the idea
# order of sums: brakets,exponents,multiply divide modular arithmatic floor division,addition subraction,comparison operators e.g. <,equality operators e.g. ==, finally the funky shortened sum things e.g. +=

# simple rng,note can only produce positive random numbers
def randnm(x,m) #x can be any pseudo random number,dice the time in milliseconds etc.,m is maximum value from rng
  #parameters of rng
  x=       #meme;this is essentially a seed for the rng
  m=       #note;m should be the smallest value here
  global rngout
  a = (x * rngout) + (x  * m)
  rngout = a % m
  #rngout is the random number produced(note give an initial value as a global variable to r as a  global variable think of it as a second seed)

#reLU, most efficient activation algorithm for hidden layers
def reLU(x,a)
  global reLUout
  if x > 0
    reLUout = x
  else
    a = 0.01  #zero for normal reLU, small number for leaky reLU,keep it as a learned parameter for Para Relu(effective not efficient,evolution may be the easiest way to implement)
    reLUout = x*a #you can change this formula if you want something different where x is negative, you can study how different functions here affect things

def eulerno(x) #note:run this function once to give a value to euler's number note:both e and x are meme
  global eulern #euler's number or e
  eulern = (1+1/x)**x

def sigmoid(x)#good output function for classsification problems with multiple outputs)
  global eulern #requires euler's number
  global sigout 
  var = 1/30 #anti-meme, can be adjusted with learning or evolution
  sigout = 1 / ( 1 + eulern ** ( var * x * (-1) ) )

