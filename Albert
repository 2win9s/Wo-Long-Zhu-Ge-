def random (x,m):

  x = 1
  m = 1
  global rngout
  a = (x * rngout) + (x * m)
  rngout = a % m
    
    #reLU, most efficient activation algorithm for hidden layers
def reLU(x,a):
  global reLUout
  if x > 0:
    reLUout = x
  else:
    a = 0.01                                      #zero for normal reLU, small number for leaky reLU,keep it as a learned parameter for Para Relu(effective not efficient,evolution may a good way to implement if there are  other parameterss that would also evolve)
    reLUout = x*a                                 #you can change this formula if you want something different where x is negative, you can study how different functions here affect things

def eulerno(x)                                    #note:run this function once to give a value to euler's number note:both e and x are meme
  global eulern                                   #euler's number or e
  eulern = (1+1/x)**x
  
def sigmoid(x)                                    #good output function for classsification problems with multiple outputs)
  global eulern                                   #requires euler's number
  global sigout 
  var = 1/5                                       #anti-meme, can be adjusted with learning or evolution,note: var > 1 is risky and possibly very ineffective,can ruin your entire network
  sigout = 1 / ( 1 + ( eulern ** ( var * x * (-1) ) ) )

input = []
def inputgen(x)                                   #adds elements to the input layer, x is the number of elements in input layer
  for length in range(x)
    input.append(0)
    
def inputwrite(x,y)                               #assigns a value y to element x of the input,note:lists start at 0 so first element is 0 2nd 1 one etc.
  global input
  input[x] = y
  
def hiddenlayergen(x)
  numofnu = []                                    #create a list of the number of neurons in each layer here
  for number in range(x)
    global hiddenlayers
    hiddenlayers.append([])
    for neurons in range(numofnu[x])
      hiddenlayers[x].append(0)
  
output = []
def outputgen(x)                                  #adds elements to the output layer, x is the number of elements in output layer
  for length in range(x)
    output.append(0)
    
def weightgen()                                     #creates a list of list of weights aka fustration 
  global weights
  global hiddenlayers
  global input                                    #the number of weights depends on inputs and hidden layers
  weights = []
  k = len(hiddenlayers)+1                         #there is one extra layer of weights because of output
  for amount in range(k)
    weights.append([])                            #creates the number of sets of weights
  for inputweights in range(len(input))           # the special set of weights from input to first hidden layer
    weights[0].append([])                         #adds a row of weights for each input
    for inputmulti in range(len(hiddenlayers[0]))
      weights[0][inputweights]
        .append(0)          #adds weights to each row based on hidden layer
  for x in range(len(hiddenlayers))
     y = x + 1
     if y < len(hiddenlayers)
      for z in range(len(hiddenlayers[x]))         
        weights[y].append([])                     #each matrix of weights has a number of rows
        for a in range(len(hiddenlayers[y]))
           weights[y][z].append(0)                #each row has a number of weights
     else
       for z in range(len(hiddenlayers[-1])) 
        weights[-1].append([])                    #special case for output 
        global output
        for a in range(len(output))
           weights[-1][z].append(0)
         
def biasgen()                                       #generates the list of biases
  global bias
  bias = []
  global hiddenlayers
  global output
  bias = hiddenlayers.copy()
  bias.append(output)
  
def squish()                                        #so this squishes a 2-d matrix of number from multiplying the input to weights to get the next layer
  global placeholder
  global placeholderz
  for x in range(len(placeholder[0]))
    for y in range(len(placeholder))
      placeholderz[x] = placeholder[y][x] + placeholderz[x]

def clear()                                         #resets the placeholder lists
  global placeholder
  global placeholderz
  placeholder.clear()
  placeholderz.clear()
def set()                                           #sets up the placeholders
  global placeholder
  global placeholderz
  
  placeholder.append([])
  placeholderz.append(0)
  
def reLUed()                                        #uses the reLU function on placeholderz
  for ez in range(len(placeholderz))
        global reLUout
        reLU(placeholderz[ez],0.01)
        placeholderz[ez] = reLUout

def sigs()
   for ez in range(len(placeholderz))
        global sigout
        sigmoid(placeholderz[ez])
        placeholderz[ez] = sigout
  
  
